{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhaltliche Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir kennen nun die Statistik der Metadaten und einige Korrelationen, die uns zuversichtlich stimmen, dass der Transport-Flair des Technology-Subreddit gut geeignet für unsere Analyse ist.\n",
    "\n",
    "Allerdings müssen wir noch die inhaltliche Seite überprüfen. So wäre es z.B. möglich, dass das Reddit voller Spam-Nachrichten ist oder die Diskussion trotz des Namens in eine völlig andere Richtung gehen. Dazu müssen wir die Texte analysieren.\n",
    "\n",
    "Wir laden zunächst die Title und Texte aus der Datenbank ein: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f transport-all-comments.csv.xz || wget  https://github.com/heiseacademy/ml-python/blob/main/06-ml-projekt/transport-all-comments.csv.xz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(\"transport-all-comments.csv.xz\", parse_dates=[\"created_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die einzelnen Wörter zu zählen, ist der `Counter` aus dem `collections`-Paket von Python optimal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir betrachten zunächst die Titel und müssen diese nun in Wörter zerlegen. [Tokenisierung](https://de.wikipedia.org/wiki/Tokenisierung) ist ein nicht-triviales Problem, das man normalerweise mit spezieller Software wie etwas [spaCy](https://spacy.io) lösen sollte. Das sparen wir uns allerdings hier und nutzen einen einfache `regex`-Tokenizer, weil wir sonst sehr lange auf die Ergebnisse waren müssten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install regex tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from tqdm.auto import tqdm\n",
    "title_counter = Counter([w.lower() for t in tqdm(posts[\"text\"]) for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `title_counter` verfügt über eine `most_common`-Funktion, mit der wir uns die häufigsten Wörter ausgeben lassen könnten. Stattdessen nutzen wir Wordclouds, die uns eine intuitive Visualisierung bieten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "wc.generate_from_frequencies(title_counter)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider kann man außer sehr allgemeinen Wörtern nicht viel erkennen. Wir müssen die Ergebnisse filtern und die sog. *Stoppworte* eliminieren. Zum Glück gibt es dazu fertige Listen, die wir hier noch etwas ergänzen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "for w in \"removed deleted post message account moderators http https www youtube com \\\n",
    "          watch gt look looks feel test know think go going submission link apologize \\\n",
    "          inconvenience don want automatically based buy compose good image karma like \\\n",
    "          lot need people self shit sound sounds spam submitting subreddit things \\\n",
    "          video way years time days doesn en fuck money org read reddit review \\\n",
    "          right said says subreddit subreddits sure thank try use videos wiki \\\n",
    "          wikipedia work ll thing point ve actually wait hello new amp better \\\n",
    "          isn yeah probably pretty yes didn pay long posts commenting portion \\\n",
    "          contribute questions unfortunately allowed submissions gifs pics sidebar\".split(\" \"):\n",
    "    stopwords.add(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir nutzen diese Liste und lassen einbuchstabige Wörter auch gleich weg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_counter = Counter([w for t in tqdm(posts[\"text\"].str.lower())\n",
    "                            for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)\n",
    "                               if (w not in stopwords) and (len(w) > 1)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wordcloud kann wieder genauso erzeugt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "wc.generate_from_frequencies(title_counter)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sieht schon sehr gut aus und passt genau zu unserem Thema. Wunderbar, das bedeutet, dass wir die richtige Datenmenge ausgewählt haben und auch unsere Klassifikation gut funktioniert hat.\n",
    "\n",
    "Analysieren wir zum Vergleich noch den Text der Toplevel-Posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counter = Counter([w for t in tqdm(posts[posts[\"parent_id\"].isna()][\"text\"].str.lower()) \n",
    "                            for w in re.findall(r'[\\w-]*\\p{L}[\\w-]*', t)\n",
    "                               if (w not in stopwords) and (len(w) > 1)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "wc.generate_from_frequencies(text_counter)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch das passt prima und sieht sehr ähnlich aus wie die Ergebnisse inkl. der Kommentare. In anderen Worten bedeutet das, dass die Kommentare gut zu den Posts passen - wunderbar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher haben wir die inhaltlichen Aspekte der Posts nur durch Zählen der Wörter berücksichtigt. Allerdings interessieren uns auch Nischen-Themen, die mir mithilfe sog. [Topic Modelle](https://en.wikipedia.org/wiki/Topic_model) aufdecken können.\n",
    "\n",
    "Hierbei handelt es sich um ein unüberwachtes Machine Learning-Verfahren zur Aufdeckung der latenten Struktur großer Datenmengen.\n",
    "\n",
    "Am häufigsten wird für Topic Models die sog. [LDA-Methode](https://de.wikipedia.org/wiki/Latent_Dirichlet_Allocation) eingesetzt, die mit stochastischem Sampling funktioniert. Da die Berechnung sehr lange benötigt und es sich in vielen Projekten gezeigt hat, dass die Ergebnisse des NMF-Algorithmus oft (mindestens) genauso gut sind, nutzen wir diesen.\n",
    "\n",
    "Dafür werden die Texte im ersten Schritt mit TD/IDF vektorisiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(posts['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir das Topic Model instanziieren und berechnen lassen. Bei (fast) allen Topic Models müssen wir die Anzahl der Topics vorgeben. Es gibt bestimmte Metriken wie Perplexität oder Kohärenz, mit denen sich die Güte des Modells bestimmen lässt. In unserem Fall arbeiten wir einfach mit 10 Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Berechnung dauert normalerweise keine Minute, jetzt können wir die Daten visualisieren. Dafür nutzen wir eine kleine Hilfsfunktion, die über die Topics iteriert und Wordclouds als Ergebnisse darstellt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud_topic_model_summary(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        freq = {}\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            freq[feature_names[i].replace(\" \", \"_\")] = topic[i]\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "        wc.generate_from_frequencies(freq)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns nun die Wordclouds für die 10 Topics aus dem Topic Model ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topic_model_summary(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out(), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plötzlich können wir auch Nischenthemen erkennen, die uns vorher verborgen waren. Das ist sehr praktisch, um Ideen für Trends zu identifizieren. Hiermit können wir außerdem erkennen, ob bestimmte Wörter möglicherweise noch eliminiert werden müssen (wie z.B. `deleted post`, das sich deswegen auch in den Stopwords findet).\n",
    "\n",
    "Durch die Geschwindigkeit, mit der ein NMF-Topic Model berechnet werden kann, bietet sich diese Methode auch zur Qualitätssicherung an."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
