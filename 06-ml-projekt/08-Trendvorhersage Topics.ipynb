{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trendvorhersage Topics\n",
    "\n",
    "Bisher hast du nur *extensive Größen* vorhergesagt. Es könnte sein, dass das Wachstum des Flairs nur mit dem Wachstum von Reddit alleine zusammenhängt.\n",
    "\n",
    "*Intensive Größen* haben diese Probleme nicht. Eine solche kennst du schon, nämliche die Verteilung der Topics. Dazu wendest du in diesem Notebook nochmal das Topic Model an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutzung für die Reddit-Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lade auch hier zuerst wie gewohnt die Daten in einen `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "posts = pd.read_csv(\"transport-all-comments.csv.xz\", parse_dates=[\"created_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic-Größen vorhersagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Code kennst du schon von den Topic Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstop_words\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STOP_WORDS \u001b[38;5;28;01mas\u001b[39;00m stopwords\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremoved deleted post message account moderators http https www youtube com \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m          watch gt look looks feel test know think go going submission link apologize \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m          inconvenience don want automatically based buy compose good image karma like \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m          isn yeah probably pretty yes didn pay long posts commenting portion \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m          contribute questions unfortunately allowed submissions gifs pics sidebar\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     12\u001b[0m     stopwords\u001b[38;5;241m.\u001b[39madd(w)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "for w in \"removed deleted post message account moderators http https www youtube com \\\n",
    "          watch gt look looks feel test know think go going submission link apologize \\\n",
    "          inconvenience don want automatically based buy compose good image karma like \\\n",
    "          lot need people self shit sound sounds spam submitting subreddit things \\\n",
    "          video way years time days doesn en fuck money org read reddit review \\\n",
    "          right said says subreddit subreddits sure thank try use videos wiki \\\n",
    "          wikipedia work ll thing point ve actually wait hello new amp better \\\n",
    "          isn yeah probably pretty yes didn pay long posts commenting portion \\\n",
    "          contribute questions unfortunately allowed submissions gifs pics sidebar\".split(\" \"):\n",
    "    stopwords.add(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vektorisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(posts['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-Modelle berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du verwendest die vereinfachte Darstellung der Topics als Texte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du möchtest eine Vorhersage auf Monatsbasis durchführen. Dazu musst du das Datum der einzelnen Posts auf Monate runden. Die `resample`-Funktion kannst du hier leider nicht nutzen, weil damit auch immer eine Aggregation verbunden ist, die dir hier im Wege stehen würde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"month\"] = posts.index.strftime(\"%Y-%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend verwendest du eine Schleife, um über alle Monate zu iterieren. Du wendest das Topic Model auf die Textdaten des jeweiligen Monats an - dabei hilft dir `numpy` mit komfortablen Selektionsmöglichkeiten der TF/IDF-Vektoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "month_data = []\n",
    "for month in np.unique(np.unique(posts[\"month\"])):\n",
    "    W_month = nmf_text_model.transform(tfidf_text_vectors[np.array(posts[\"month\"] == month)])\n",
    "    month_data.append([month] + list(W_month.sum(axis=0)/W_month.sum()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gib den Topics nun kurze Namen, die aus den zwei wichtigsten Wörtern bestehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = []\n",
    "voc = tfidf_text_vectorizer.get_feature_names_out()\n",
    "for topic in nmf_text_model.components_:\n",
    "    important = topic.argsort()\n",
    "    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
    "    topic_names.append(\"Topic \" + top_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ergebnisse kannst du gut in einem Area-Plot darstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month = pd.DataFrame(month_data, columns=[\"month\"] + topic_names).set_index(\"month\")\n",
    "df_month.plot.area(figsize=(16, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trendvorhersage für Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Anteil des `tesla model`-Topics kannst du nun als Größe nutzen, für die du den Trend vorhersagen möchtest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "df = pd.DataFrame({\"ds\": df_month.index.values, \n",
    "                   \"y\": df_month[\"Topic tesla model\"].values})\n",
    "\n",
    "m = Prophet()\n",
    "m.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das führst du wieder für zwei Monate in die Zukunft aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=24, freq='M')\n",
    "forecast = m.predict(future)\n",
    "fig1 = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig2 = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Tesla-Topic scheint also weiter an Gewicht zu gewinnen. Es bleibt abzuwarten, ob mehr und mehr Hersteller von Elektrofahrzeugen dort nicht noch ihren Einfluss einbringen können"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
